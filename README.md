# 摘要
> 优化算法指通过改善训练方式，来最小化(或最大化)损失函数E(x)
---
# 一阶
> 使用各参数的梯度值来最小化或最大化损失函数E(x)
# batch优化
> 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
2.此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
3.在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。
## 随机梯度下降
> 对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快。
频繁的更新使得参数间具有高方差，损失函数会以不同的强度波动。这实际上是一件好事，因为它有助于我们发现新的和可能更优的局部最小值，而标准梯度下降将只会收敛到某个局部最优值。
但SGD的问题是，由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量。
## 批量梯度下降
> 传统的批量梯度下降将计算整个数据集梯度，但只会进行一次更新，因此在处理大型数据集时速度很慢且难以控制，甚至导致内存溢出。
权重更新的快慢是由学习率η决定的，并且可以在凸面误差曲面中收敛到全局最优值，在非凸曲面中可能趋于局部最优值。
使用标准形式的批量梯度下降还有一个问题，就是在训练大型数据集时存在冗余的权重更新。
## mini-batch梯度下降
# 学习率优化
> 指数加权平均

## 动量梯度下降Momentum
## RMSprop
## Adam
## 学习率衰减
# 局部最优
# 二阶
> 使用了二阶导数(也叫做Hessian方法)来最小化或最大化损失函数。由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。
## 牛顿法
## 拟牛顿法
> 牛顿法有个缺点，海森矩阵是非稀疏矩阵，参数太多，其计算量太大。

---
参考资料
- [一文看懂各种神经网络优化算法：从梯度下降到Adam方法](http://www.sohu.com/a/149921578_610300)
- [神经网络优化算法综述](http://blog.csdn.net/young_gy/article/details/72633202)


